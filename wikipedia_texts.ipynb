{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b89dda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from wikidata.client import Client\n",
    "from itertools import islice\n",
    "import ast\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099fe226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the two dataset to csv\n",
    "silverdataset = pd.read_csv('datasets/silver.csv')\n",
    "golddataset = pd.read_csv('datasets/gold.csv')\n",
    "\n",
    "silver_links = pd.read_csv('datasets/wikipedia_links_silver.csv')\n",
    "gold_links = pd.read_csv('datasets/wikipedia_links_gold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80227659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n"
     ]
    }
   ],
   "source": [
    "#Extract totality of languages\n",
    "silver_lang = set(silver_links['language'])\n",
    "gold_lang = set(gold_links['language'].unique())\n",
    "total_lang_count = silver_lang.union(gold_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25327e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_74220/1917892312.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  silver_grouped = df_silver.groupby(\"entity\").apply(create_dic).reset_index(name=\"lang_url_map\")\n",
      "/var/folders/yq/gyf11wn125qgkzlcv24mbp3w0000gn/T/ipykernel_74220/1917892312.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gold_grouped = df_gold.groupby(\"entity\").apply(create_dic).reset_index(name=\"lang_url_map\")\n"
     ]
    }
   ],
   "source": [
    "#Group entities to construct dictionary\n",
    "# 🔁 Group by 'entity', crea un dizionario {lang: url} per ciascuna entity\n",
    "df_silver = pd.DataFrame(silver_links)\n",
    "df_gold = pd.DataFrame(gold_links)\n",
    "\n",
    "create_dic = lambda x: dict(zip(x[\"language\"], x[\"url\"]))\n",
    "silver_grouped = df_silver.groupby(\"entity\").apply(create_dic).reset_index(name=\"lang_url_map\")\n",
    "gold_grouped = df_gold.groupby(\"entity\").apply(create_dic).reset_index(name=\"lang_url_map\")\n",
    "\n",
    "silver_grouped.to_csv(\"datasets/grouped_silver_links.csv\", index=False)\n",
    "gold_grouped.to_csv(\"datasets/grouped_gold_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c013c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Vincent Lange (born 15 June 1974 in Berlin) actor, producer, sports manager and coach, ex volleyball player from Germany, who competed for the Men's National Team in the 2000s. He played as a receiver/spiker and libero.\\n\\n\\n== Honours ==\\n(Beach Volleyball Pro / International Tournaments & Top Rankings 1997 - 2001)\\n2001 European Championship — 9th place\\n2002 FIVB World League — 9th place\\n(German Champion 2003 / SCC Berlin)\\n(European Cup Champion 2006 / Copra Berni Piacenza)\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nVincent Lange at FIVB.com \\nVincent Lange at the Beach Volleyball Database\", 758.25, 512.6525992326577]\n"
     ]
    }
   ],
   "source": [
    "head = silver_grouped.head(1)\n",
    "for idx, row in head.iterrows():\n",
    "    results = [\"\"]     \n",
    "    lang_distribution = []\n",
    "    for key in list(dict.keys()):\n",
    "        text, length = extract_text(key, dict[key])\n",
    "        if (key == 'en'): #the language is english -> i can save the text for further studies\n",
    "            results[0] = text\n",
    "        lang_distribution.append(length)\n",
    "\n",
    "    #append variance and avg to results\n",
    "    l = len(lang_distribution)\n",
    "    mean = sum(lang_distribution)/l\n",
    "    results.append(mean)\n",
    "    results.append(math.sqrt(sum((x - mean) ** 2 for x in lang_distribution) / l))  # Popolazione)\n",
    "    print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca197d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(lang, wikipedia_url):\n",
    "    \n",
    "    title = wikipedia_url.split('/')[-1]\n",
    "    #print(\"english wikipedia title:\", title)\n",
    "    lang = lang.replace('_', '-')\n",
    "\n",
    "    # Use Wikipedia's API to get plain text of the article\n",
    "    api_url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = requests.get(api_url, params=params, timeout=3).json()\n",
    "        page = next(iter(res[\"query\"][\"pages\"].values())) #see below the dictionary, we create an iterable and pick the first and only item\n",
    "        text = page.get(\"extract\", \"\") # we get the \"extract field\"\n",
    "        if lang == 'en':\n",
    "            return (text, len(text))\n",
    "        return (\"\", len(text))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Errore nella richiesta: {e} per link {wikipedia_url}\")\n",
    "        return (\"\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e72c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "# Funzione per salvare progressivamente i risultati in un file CSV\n",
    "def save_wikipedia_text(file_name):\n",
    "    output_file_name = \"wikipedia_text_stats_\"+file_name\n",
    "    # Controlla se il file di output esiste già\n",
    "    if os.path.exists(output_file_name):\n",
    "        # Carica il file esistente\n",
    "        existing_df = pd.read_csv(output_file_name)\n",
    "        # Raccoglie le keyword già trattate\n",
    "        processed_keywords = existing_df['entity'].unique().tolist()\n",
    "        print(f\"🌍 {len(processed_keywords)} keywords già trattate.\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=['entity', 'engtext', 'distribution', 'std', 'avg'])\n",
    "        processed_keywords = []\n",
    "    \n",
    "    # Carica il dataset di input\n",
    "    data = pd.read_csv(\"datasets/\"+file_name)\n",
    "    df = pd.DataFrame(data, columns=['entity', 'lang_url_map']) #refers to the wikipedia url\n",
    "\n",
    "    # Lista per raccogliere i risultati\n",
    "    results = [None, \"\"]\n",
    "\n",
    "    # Cicla su ogni entità\n",
    "    for idx, row in df.iterrows():\n",
    "        #print(row)\n",
    "        item = row['entity']\n",
    "        dict = ast.literal_eval(row['lang_url_map']) #ast converts string to dictionary\n",
    "        results[0] = item\n",
    "        \n",
    "        # Se la keyword è già stata trattata, salta\n",
    "        if item in processed_keywords:\n",
    "            print(f\"🔁 '{item}' già trattata, salto.\")\n",
    "            continue\n",
    "        print(f\"Elaboro: {item}\")\n",
    "\n",
    "        lang_distribution = []\n",
    "        for key in list(dict.keys()):\n",
    "            #print(\"working with: \", key, dict[key])\n",
    "            text, length = extract_text(key, dict[key])\n",
    "            if length == 0:\n",
    "                continue\n",
    "            if (key == 'en'): #the language is english -> i can save the text for further studies\n",
    "                clean_text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "                results[1] = [clean_text]\n",
    "            lang_distribution.append(length)\n",
    "\n",
    "        #append lang_distribution\n",
    "        results.append(lang_distribution)\n",
    "\n",
    "        #append std and avg to results\n",
    "        l = len(lang_distribution)\n",
    "        mean = sum(lang_distribution)/l\n",
    "        results.append(math.sqrt(sum((x - mean) ** 2 for x in lang_distribution) / l))  # Popolazione)\n",
    "        results.append(mean)\n",
    "        \n",
    "        # Scrivi progressivamente i risultati\n",
    "        with open(output_file_name, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not os.path.exists(output_file_name):\n",
    "                writer.writerow(['entity', 'engtext', 'distribution', 'std', 'avg'])  # o dinamico, se serve\n",
    "            writer.writerow(results)\n",
    "        results = [None, \"\"]  # Resetta la lista dei risultati per non aggiungere più volte\n",
    "        \n",
    "        # Aggiungi la keyword alla lista delle trattate\n",
    "        processed_keywords.append(item)\n",
    "\n",
    "    print(f\"✅ Processamento completato. Risultati salvati in '{output_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 24 keywords già trattate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-26:\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]Process SpawnProcess-25:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/process.py\", line 246, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_entity' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/process.py\", line 246, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_entity' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/process.py\", line 246, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_entity' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-28:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/process.py\", line 246, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_entity' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/process.py\", line 246, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_entity' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m#entity, engtext, distribution, std, avg\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43msave_wikipedia_text_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrouped_gold_links.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36msave_wikipedia_text_parallel\u001b[39m\u001b[34m(file_name)\u001b[39m\n\u001b[32m     56\u001b[39m future_to_entity = {\n\u001b[32m     57\u001b[39m     executor.submit(process_entity, row, processed_keywords): row[\u001b[33m'\u001b[39m\u001b[33mentity\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows()\n\u001b[32m     59\u001b[39m }\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(as_completed(future_to_entity), total=\u001b[38;5;28mlen\u001b[39m(future_to_entity)):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     result = \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# Scrivi subito sul file ogni risultato ricevuto\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_name, mode=\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, newline=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def process_entity(row, processed_keywords):\n",
    "    item = row['entity']\n",
    "    dict_url = ast.literal_eval(row['lang_url_map'])\n",
    "\n",
    "    if item in processed_keywords:\n",
    "        return None\n",
    "\n",
    "    print(f\"⚙️ Processando '{item}'\")\n",
    "    results = [item, \"\"]\n",
    "    lang_distribution = []\n",
    "\n",
    "    for key in dict_url:\n",
    "        text, length = extract_text(key, dict_url[key])\n",
    "        if length == 0:\n",
    "            continue\n",
    "        if key == 'en':\n",
    "            clean_text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "            results[1] = clean_text\n",
    "        lang_distribution.append(length)\n",
    "\n",
    "    if not lang_distribution:\n",
    "        return None\n",
    "\n",
    "    l = len(lang_distribution)\n",
    "    mean = sum(lang_distribution) / l\n",
    "    std = math.sqrt(sum((x - mean) ** 2 for x in lang_distribution) / l)\n",
    "\n",
    "    results.append(lang_distribution)\n",
    "    results.append(std)\n",
    "    results.append(mean)\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_wikipedia_text_parallel(file_name):\n",
    "    output_file_name = \"wikipedia_text_stats_\" + file_name\n",
    "\n",
    "    # Carica keyword già trattate\n",
    "    if os.path.exists(output_file_name):\n",
    "        existing_df = pd.read_csv(output_file_name)\n",
    "        processed_keywords = existing_df['entity'].unique().tolist()\n",
    "        print(f\"🌍 {len(processed_keywords)} keywords già trattate.\")\n",
    "    else:\n",
    "        processed_keywords = []\n",
    "        # Scriviamo l'header se il file non esiste\n",
    "        with open(output_file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['entity', 'engtext', 'distribution', 'std', 'avg'])\n",
    "\n",
    "    data = pd.read_csv(\"datasets/\" + file_name)\n",
    "    df = pd.DataFrame(data, columns=['entity', 'lang_url_map'])\n",
    "\n",
    "    # Parallelizza il processo\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_entity = {\n",
    "            executor.submit(process_entity, row, processed_keywords): row['entity']\n",
    "            for _, row in df.iterrows()\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(future_to_entity), total=len(future_to_entity)):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                # Scrivi subito sul file ogni risultato ricevuto\n",
    "                with open(output_file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(result)\n",
    "\n",
    "    print(f\"✅ Processamento completato. Risultati salvati in '{output_file_name}'.\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity, engtext, distribution, std, avg\n",
    "if __name__ == '__main__':\n",
    "    save_wikipedia_text_parallel('grouped_gold_links.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093e77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
