{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1046bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi il dataset originale una sola volta\n",
    "silver_dataset = pd.read_csv('../datasets/silver_unicum.csv')\n",
    "gold_dataset = pd.read_csv('../datasets/gold_unicum.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_quantili(train_dataset, val_dataset, in_column_name, out_column_name, n_quantili):\n",
    "    #Taken a numerical column as input, <n_quantili> bins are created. \n",
    "    #The \"quantilization\" is performed based just on train_dataset values, to avoid data leakage\n",
    "    #The bins are then applied both to the train and val dataset\n",
    "    #Finally the old numerical columns are dropped\n",
    "    \n",
    "    bins_edges = pd.qcut(train_dataset[in_column_name], q=n_quantili, retbins=True)[1]\n",
    "    train_dataset[out_column_name] = pd.cut(train_dataset[in_column_name], bins=bins_edges, include_lowest=True, duplicates='drop')\n",
    "    val_dataset[out_column_name] = pd.cut(val_dataset[in_column_name], bins=bins_edges, include_lowest=True, duplicates='drop')\n",
    "\n",
    "    train_dataset.drop(columns=in_column_name, inplace=True)\n",
    "    val_dataset.drop(columns=in_column_name, inplace=True)\n",
    "\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc308dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If entry == something i just punt one, regarding of the content\n",
    "str_to_one = lambda x: 0 if len(x) == 1 else 1\n",
    "silver_dataset['countryLabel'] = silver_dataset['countryLabel'].apply(str_to_one)\n",
    "silver_dataset['subclass_ofLabel'] = silver_dataset['subclass_ofLabel'].apply(str_to_one)\n",
    "gold_dataset['countryLabel'] = gold_dataset['countryLabel'].apply(str_to_one)\n",
    "gold_dataset['subclass_ofLabel'] = gold_dataset['subclass_ofLabel'].apply(str_to_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['label_cultural agnostic', 'label_cultural exclusive', 'label_cultural representative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffea56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silver_dataset.columns)\n",
    "print(gold_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "columns_to_quantili = ['len']\n",
    "\n",
    "unnecessary_columns =  [ 'item', 'name', 'text_distribution', 'ref_distribution', 'std_text', 'avg_text', 'entropy_text', 'gini_text', 'sum_over_texts',\n",
    "                         'sum_over_ref', 'entropy_ref', 'gini_ref', \n",
    "                        'h_nat_engtext'\n",
    "                    ]\n",
    "silver_dataset = silver_dataset.drop(columns=unnecessary_columns)\n",
    "gold_dataset = gold_dataset.drop(columns=unnecessary_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ff77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide numerical features in classes based on quantili\n",
    "\n",
    "for feature in columns_to_quantili:\n",
    "    silver_dataset, gold_dataset = to_quantili(silver_dataset, gold_dataset, feature, \"bin_\"+feature, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daffed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silver_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier  # Cambiato da RandomForest\n",
    "\n",
    "categorical_columns = silver_dataset.columns\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit(silver_dataset[categorical_columns])\n",
    "encoded_train = encoder.transform(silver_dataset[categorical_columns])\n",
    "encoded_eval = encoder.transform(gold_dataset[categorical_columns])\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_columns), index=silver_dataset.index)\n",
    "encoded_eval_df = pd.DataFrame(encoded_eval, columns=encoder.get_feature_names_out(categorical_columns), index=gold_dataset.index)\n",
    "\n",
    "training_dataset = silver_dataset.drop(columns=categorical_columns)\n",
    "training_dataset = pd.concat([training_dataset, encoded_train_df], axis=1)\n",
    "evaluation_dataset = gold_dataset.drop(columns=categorical_columns)\n",
    "evaluation_dataset = pd.concat([evaluation_dataset, encoded_eval_df], axis=1)\n",
    "\n",
    "X_train = training_dataset.drop(columns=label_columns)\n",
    "y_train = training_dataset[label_columns]\n",
    "X_test = evaluation_dataset.drop(columns=label_columns)\n",
    "y_test = evaluation_dataset[label_columns]\n",
    "\n",
    "# Usa ExtraTrees invece di RandomForest\n",
    "et_model = ExtraTreesClassifier(n_estimators=10000, random_state=42)\n",
    "et_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = et_model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred, average='weighted')\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"ACCURACY: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALITICS\n",
    "\n",
    "# Crea i nomi delle feature (se non li hai gi√†)\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Calcola le importanze medie e deviazione standard tra tutti gli alberi\n",
    "importances = et_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in et_model.estimators_], axis=0)\n",
    "\n",
    "# Serie ordinata con le feature\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# # Ordina e prendi le top 20\n",
    "top_n = 30\n",
    "# print(len(forest_importances))\n",
    "top_features = forest_importances.sort_values(ascending=False).head(top_n)\n",
    "# worst_features = forest_importances.sort_values(ascending=True).head(10).index.tolist()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "top_features.plot.bar(yerr=std[[feature_names.index(f) for f in top_features.index]], ax=ax)\n",
    "ax.set_title(\"Top {top_n} Feature Importances (MDI)\")\n",
    "ax.set_ylabel(\"Mean Decrease in Impurity\")\n",
    "ax.set_xlabel(\"Feature\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
