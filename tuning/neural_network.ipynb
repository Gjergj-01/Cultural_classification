{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3bc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1046bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi il dataset originale una sola volta\n",
    "silver_dataset = pd.read_csv('../datasets/silver_unicum.csv')\n",
    "gold_dataset = pd.read_csv('../datasets/gold_unicum.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc4a737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_quantili(train_dataset, val_dataset, in_column_name, out_column_name, n_quantili):\n",
    "    #Taken a numerical column as input, <n_quantili> bins are created. \n",
    "    #The \"quantilization\" is performed based just on train_dataset values, to avoid data leakage\n",
    "    #The bins are then applied both to the train and val dataset\n",
    "    #Finally the old numerical columns are dropped\n",
    "    \n",
    "    bins_edges = pd.qcut(train_dataset[in_column_name], q=n_quantili, retbins=True)[1]\n",
    "    train_dataset[out_column_name] = pd.cut(train_dataset[in_column_name], bins=bins_edges, include_lowest=True, duplicates='drop')\n",
    "    val_dataset[out_column_name] = pd.cut(val_dataset[in_column_name], bins=bins_edges, include_lowest=True, duplicates='drop')\n",
    "\n",
    "    train_dataset.drop(columns=in_column_name, inplace=True)\n",
    "    val_dataset.drop(columns=in_column_name, inplace=True)\n",
    "\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc308dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If entry == something i just punt one, regarding of the content\n",
    "str_to_one = lambda x: 0 if len(x) == 1 else 1\n",
    "silver_dataset['countryLabel'] = silver_dataset['countryLabel'].apply(str_to_one)\n",
    "silver_dataset['subclass_ofLabel'] = silver_dataset['subclass_ofLabel'].apply(str_to_one)\n",
    "gold_dataset['countryLabel'] = gold_dataset['countryLabel'].apply(str_to_one)\n",
    "gold_dataset['subclass_ofLabel'] = gold_dataset['subclass_ofLabel'].apply(str_to_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ac3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['label_cultural agnostic', 'label_cultural exclusive', 'label_cultural representative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffea56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['item', 'name', 'type', 'category', 'subcategory', 'label',\n",
      "       'countryLabel', 'subclass_ofLabel', 'text_distribution', 'std_text',\n",
      "       'avg_text', 'len', 'entropy_text', 'gini_text', 'sum_over_texts',\n",
      "       'ref_distribution', 'std_ref', 'avg_ref', 'sum_over_ref', 'entropy_ref',\n",
      "       'gini_ref', 'h_adj_descr', 'h_nat_descr', 'h_adj_engtext',\n",
      "       'h_nat_engtext'],\n",
      "      dtype='object')\n",
      "Index(['item', 'name', 'type', 'category', 'subcategory', 'label',\n",
      "       'countryLabel', 'subclass_ofLabel', 'text_distribution', 'std_text',\n",
      "       'avg_text', 'len', 'entropy_text', 'gini_text', 'sum_over_texts',\n",
      "       'ref_distribution', 'std_ref', 'avg_ref', 'sum_over_ref', 'entropy_ref',\n",
      "       'gini_ref', 'h_adj_descr', 'h_nat_descr', 'h_adj_engtext',\n",
      "       'h_nat_engtext'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(silver_dataset.columns)\n",
    "print(gold_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b0701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "columns_to_quantili = ['len']\n",
    "\n",
    "unnecessary_columns =  [ 'item', 'name', 'text_distribution', 'ref_distribution', 'std_text', 'avg_text', 'entropy_text', 'gini_text', 'sum_over_texts',\n",
    "                         'sum_over_ref', 'entropy_ref', 'gini_ref', 'h_adj_engtext',\n",
    "                        'h_nat_engtext'\n",
    "                    ]\n",
    "silver_dataset = silver_dataset.drop(columns=unnecessary_columns)\n",
    "gold_dataset = gold_dataset.drop(columns=unnecessary_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "820ff77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide numerical features in classes based on quantili\n",
    "\n",
    "for feature in columns_to_quantili:\n",
    "    silver_dataset, gold_dataset = to_quantili(silver_dataset, gold_dataset, feature, \"bin_\"+feature, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5daffed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['type', 'category', 'subcategory', 'label', 'countryLabel',\n",
      "       'subclass_ofLabel', 'std_ref', 'avg_ref', 'h_adj_descr', 'h_nat_descr',\n",
      "       'bin_len'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(silver_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf33f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY (Neural Net): 0.6187290969899666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier  # Cambiato da RandomForest\n",
    "\n",
    "categorical_columns = silver_dataset.columns\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit(silver_dataset[categorical_columns])\n",
    "encoded_train = encoder.transform(silver_dataset[categorical_columns])\n",
    "encoded_eval = encoder.transform(gold_dataset[categorical_columns])\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_columns), index=silver_dataset.index)\n",
    "encoded_eval_df = pd.DataFrame(encoded_eval, columns=encoder.get_feature_names_out(categorical_columns), index=gold_dataset.index)\n",
    "\n",
    "training_dataset = silver_dataset.drop(columns=categorical_columns)\n",
    "training_dataset = pd.concat([training_dataset, encoded_train_df], axis=1)\n",
    "evaluation_dataset = gold_dataset.drop(columns=categorical_columns)\n",
    "evaluation_dataset = pd.concat([evaluation_dataset, encoded_eval_df], axis=1)\n",
    "\n",
    "X_train = training_dataset.drop(columns=label_columns)\n",
    "y_train = training_dataset[label_columns]\n",
    "X_test = evaluation_dataset.drop(columns=label_columns)\n",
    "y_test = evaluation_dataset[label_columns]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # puoi modificarli a piacere\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,+\n",
    "    59640\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "\n",
    "score = f1_score(y_test, y_pred, average='weighted')\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"ACCURACY (Neural Net):\", acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
