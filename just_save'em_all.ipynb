{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "665a7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7dc8bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "silverdataset = pd.read_csv('datasets/silver.csv')\n",
    "golddataset = pd.read_csv('datasets/gold.csv')\n",
    "\n",
    "text_silver = pd.read_csv('datasets/wikipedia_text_stats_grouped_silver_links.csv')\n",
    "text_gold = pd.read_csv('datasets/wikipedia_text_stats_grouped_gold_links.csv')\n",
    "\n",
    "ref_silver = pd.read_csv('datasets/wikipedia_references_stats_grouped_silver_links.csv')\n",
    "ref_gold = pd.read_csv('datasets/wikipedia_references_stats_grouped_gold_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f1d99432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_from_text(column):\n",
    "    nations = set(pd.read_csv('datasets/national_adjectives.csv')[\"Country\"])\n",
    "    national_adjectives = set(pd.read_csv('datasets/national_adjectives.csv')[\"Adjective\"])\n",
    "\n",
    "    column['new_description'] = column.apply(lambda elem: None if type(elem.iloc[0]) is float else set(elem.iloc[0].split()), axis=1)\n",
    "\n",
    "    column['length_description_intersection'] = column['new_description'].apply(lambda elem: None if elem is None else len(elem.intersection(national_adjectives)))\n",
    "    \n",
    "    column['nations'] = column['new_description'].apply(lambda elem: None if elem is None else len(elem.intersection(nations)))\n",
    "\n",
    "    return column['length_description_intersection'], column['nations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d52b63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(p):\n",
    "    p = np.array(ast.literal_eval(p))\n",
    "    return 1 - np.sum(p ** 2)\n",
    "\n",
    "def entropy(p):\n",
    "    p = np.array(ast.literal_eval(p))\n",
    "    p_log2_p = np.where(p > 0, p * np.log2(p), 0)\n",
    "    return -np.sum(p_log2_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bec42143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['entity', 'engtext', 'distribution', 'std', 'avg', 'len',\n",
      "       'entropy_text', 'gini_text', 'sum_over_texts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# text_silver e text_gold preprocessing\n",
    "silver_text_dataset = pd.DataFrame(text_silver)\n",
    "#silver_text_dataset = silver_text_dataset.drop(columns=[\"engtext\"])\n",
    "gold_text_dataset = pd.DataFrame(text_gold)\n",
    "#gold_text_dataset = gold_text_dataset.drop(columns=[\"engtext\"])\n",
    "\n",
    "array_augmentation = 1\n",
    "assert(array_augmentation == 1)\n",
    "\n",
    "#Compute size of distribution array which equals to number of wikipedia links per item. (There is one per language)\n",
    "compute_len = lambda x: len(array_augmentation*ast.literal_eval(x)) if type(ast.literal_eval(x)) is not float else None\n",
    "silver_text_dataset[\"len\"] = silver_text_dataset[\"distribution\"].apply(compute_len)\n",
    "gold_text_dataset[\"len\"] = gold_text_dataset[\"distribution\"].apply(compute_len)\n",
    "\n",
    "#Saving entropy and gini informations\n",
    "silver_text_dataset[\"entropy_text\"] = silver_text_dataset[\"distribution\"].apply(entropy)\n",
    "silver_text_dataset[\"gini_text\"] = silver_text_dataset[\"distribution\"].apply(gini_index)\n",
    "\n",
    "gold_text_dataset[\"entropy_text\"] = gold_text_dataset[\"distribution\"].apply(entropy)\n",
    "gold_text_dataset[\"gini_text\"] = gold_text_dataset[\"distribution\"].apply(gini_index)\n",
    "\n",
    "#Compute sum over the distribution array\n",
    "compute_sum = lambda x: sum(array_augmentation*ast.literal_eval(x)) if type(ast.literal_eval(x)) is not float else None\n",
    "silver_text_dataset[\"sum_over_texts\"] = silver_text_dataset[\"distribution\"].apply(compute_sum)\n",
    "gold_text_dataset[\"sum_over_texts\"] = gold_text_dataset[\"distribution\"].apply(compute_sum)\n",
    "\n",
    "# silver_text_dataset = silver_text_dataset.drop(columns=\"distribution\")\n",
    "# gold_text_dataset = gold_text_dataset.drop(columns=\"distribution\")\n",
    "\n",
    "# n_quantili = 5\n",
    "\n",
    "# # avg, std, len are categorized based on quantili\n",
    "# avg_bins_edges = pd.qcut(silver_text_dataset['avg'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "# std_bins_edges = pd.qcut(silver_text_dataset['std'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "# len_bins_edges = pd.qcut(silver_text_dataset['len'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "# sum_bins_edges = pd.qcut(silver_text_dataset['sum_over_texts'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "\n",
    "# silver_text_dataset['avg_bins'] = pd.cut(silver_text_dataset['avg'], bins=avg_bins_edges, include_lowest=True)\n",
    "# silver_text_dataset['std_bins'] = pd.cut(silver_text_dataset['std'], bins=std_bins_edges, include_lowest=True)\n",
    "# silver_text_dataset['len_bins'] = pd.cut(silver_text_dataset['len'], bins=len_bins_edges, include_lowest=True)\n",
    "# silver_text_dataset['texts_sum'] = pd.cut(silver_text_dataset['sum_over_texts'], bins=sum_bins_edges, include_lowest=True)\n",
    "# silver_text_dataset.dropna(subset=[\"len_bins\"], inplace=True)\n",
    "# silver_text_dataset.dropna(subset=[\"texts_sum\"], inplace=True)\n",
    "\n",
    "# gold_text_dataset['avg_bins'] = pd.cut(gold_text_dataset['avg'], bins=avg_bins_edges, include_lowest=True)\n",
    "# gold_text_dataset['std_bins'] = pd.cut(gold_text_dataset['std'], bins=std_bins_edges, include_lowest=True)\n",
    "# gold_text_dataset['len_bins'] = pd.cut(gold_text_dataset['len'], bins=len_bins_edges, include_lowest=True)\n",
    "# gold_text_dataset['texts_sum'] = pd.cut(gold_text_dataset['sum_over_texts'], bins=sum_bins_edges, include_lowest=True)\n",
    "# gold_text_dataset.dropna(subset=[\"len_bins\"], inplace=True)\n",
    "# gold_text_dataset.dropna(subset=[\"texts_sum\"], inplace=True)\n",
    "\n",
    "# #avg, std, len are dropped\n",
    "# silver_text_dataset = silver_text_dataset.drop(columns=[\"avg\", \"std\", \"len\", \"sum_over_texts\"])\n",
    "# gold_text_dataset = gold_text_dataset.drop(columns=[\"avg\", \"std\", \"len\", \"sum_over_texts\"])\n",
    "\n",
    "print(silver_text_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d53e6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['item', 'name', 'description', 'type', 'category', 'subcategory',\n",
      "       'label', 'engtext', 'distribution', 'std', 'avg', 'len', 'entropy_text',\n",
      "       'gini_text', 'sum_over_texts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Now we want to combine [silverdataset, silver_text_dataset] and [goldataset, gold_text_dataset] with respect to the key <entity>\n",
    "silver_merged = pd.merge(silverdataset, silver_text_dataset, left_on='item', right_on='entity')\n",
    "gold_merged = pd.merge(golddataset, gold_text_dataset, left_on='item', right_on='entity')\n",
    "\n",
    "silver_merged = silver_merged.drop(columns=\"entity\")\n",
    "gold_merged = gold_merged.drop(columns=\"entity\") \n",
    "\n",
    "print(silver_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a8774502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['entity', 'ref_distribution', 'std_ref', 'avg_ref', 'sum_over_ref',\n",
      "       'entropy_ref', 'gini_ref'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# text_silver e text_gold preprocessing\n",
    "silver_ref_dataset = pd.DataFrame(ref_silver)\n",
    "gold_ref_dataset = pd.DataFrame(ref_gold)\n",
    "\n",
    "array_augmentation = 1\n",
    "assert(array_augmentation == 1)\n",
    "\n",
    "#Compute sum over the distribution array\n",
    "compute_sum = lambda x: sum(array_augmentation*ast.literal_eval(x)) if type(ast.literal_eval(x)) is not float else None\n",
    "silver_ref_dataset[\"sum_over_ref\"] = silver_ref_dataset[\"ref_distribution\"].apply(compute_sum)\n",
    "gold_ref_dataset[\"sum_over_ref\"] = gold_ref_dataset[\"ref_distribution\"].apply(compute_sum)\n",
    "\n",
    "#Saving entropy and gini informations\n",
    "silver_ref_dataset[\"entropy_ref\"] = silver_ref_dataset[\"ref_distribution\"].apply(entropy)\n",
    "silver_ref_dataset[\"gini_ref\"] = silver_ref_dataset[\"ref_distribution\"].apply(gini_index)\n",
    "\n",
    "gold_ref_dataset[\"entropy_ref\"] = gold_ref_dataset[\"ref_distribution\"].apply(entropy)\n",
    "gold_ref_dataset[\"gini_ref\"] = gold_ref_dataset[\"ref_distribution\"].apply(gini_index)\n",
    "\n",
    "# silver_ref_dataset = silver_ref_dataset.drop(columns=\"ref_distribution\")\n",
    "# gold_ref_dataset = gold_ref_dataset.drop(columns=\"ref_distribution\")\n",
    "\n",
    "# n_quantili = 5\n",
    "\n",
    "# # avg, std, len are categorized based on quantili\n",
    "# avg_bins_edges = pd.qcut(silver_ref_dataset['avg_ref'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "# std_bins_edges = pd.qcut(silver_ref_dataset['std_ref'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "# sum_bins_edges = pd.qcut(silver_ref_dataset['sum_over_ref'], q=n_quantili, retbins=True)[1]  # 5 intervalli uguali\n",
    "\n",
    "# silver_ref_dataset['avg_ref_bins'] = pd.cut(silver_ref_dataset['avg_ref'], bins=avg_bins_edges, include_lowest=True)\n",
    "# silver_ref_dataset['std_ref_bins'] = pd.cut(silver_ref_dataset['std_ref'], bins=std_bins_edges, include_lowest=True)\n",
    "# silver_ref_dataset['ref_sum'] = pd.cut(silver_ref_dataset['sum_over_ref'], bins=sum_bins_edges, include_lowest=True)\n",
    "# silver_ref_dataset.dropna(subset=[\"ref_sum\"], inplace=True)\n",
    "\n",
    "# gold_ref_dataset['avg_ref_bins'] = pd.cut(gold_ref_dataset['avg_ref'], bins=avg_bins_edges, include_lowest=True)\n",
    "# gold_ref_dataset['std_ref_bins'] = pd.cut(gold_ref_dataset['std_ref'], bins=std_bins_edges, include_lowest=True)\n",
    "# gold_ref_dataset['ref_sum'] = pd.cut(gold_ref_dataset['sum_over_ref'], bins=sum_bins_edges, include_lowest=True)\n",
    "# gold_ref_dataset.dropna(subset=[\"ref_sum\"], inplace=True)\n",
    "\n",
    "# #avg, std, len are dropped\n",
    "# silver_ref_dataset = silver_ref_dataset.drop(columns=[\"avg_ref\", \"std_ref\", \"sum_over_ref\"])\n",
    "# gold_ref_dataset = gold_ref_dataset.drop(columns=[\"avg_ref\", \"std_ref\", \"sum_over_ref\"])\n",
    "\n",
    "print(silver_ref_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ffde3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to combine [silverdataset, silver_ref_dataset] and [goldataset, gold_ref_dataset] with respect to the key <entity>\n",
    "silver_merged = pd.merge(silver_merged, silver_ref_dataset, left_on='item', right_on='entity')\n",
    "gold_merged = pd.merge(gold_merged, gold_ref_dataset, left_on='item', right_on='entity')\n",
    "\n",
    "silver_merged = silver_merged.drop(columns=\"entity\")\n",
    "gold_merged = gold_merged.drop(columns=\"entity\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e0fcb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = silver_merged\n",
    "evaluation_data = gold_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c5eb71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rimuovo colonne non utili dal training set e dall'evaluation test\n",
    "dataset = training_data.drop(columns=[\"description\", \"engtext\"])\n",
    "evaluation_dataset = evaluation_data.drop(columns=[\"description\", \"engtext\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a989e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING SU DESCRIPTION\n",
    "# Heuristic su 'description'\n",
    "train_desc = pd.DataFrame(training_data[\"description\"])\n",
    "dataset['h_adj_descr'], dataset['h_nat_descr'] = heuristic_from_text(train_desc)\n",
    "\n",
    "# Stesso preprocessing per l'evaluation set\n",
    "eval_desc = pd.DataFrame(evaluation_data[\"description\"])\n",
    "evaluation_dataset['h_adj_descr'], evaluation_dataset['h_nat_descr'] = heuristic_from_text(eval_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8da97e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING SU ENGTEXT\n",
    "#Heuristic su engtext\n",
    "train_engtext = pd.DataFrame(training_data[\"engtext\"])\n",
    "dataset['h_adj_engtext'], dataset['h_nat_engtext'] = heuristic_from_text(train_engtext)\n",
    "dataset.dropna(subset=[\"h_adj_engtext\", \"h_nat_engtext\"], inplace=True)\n",
    "\n",
    "# Stesso preprocessing per l'evaluation set\n",
    "eval_engtext = pd.DataFrame(evaluation_data[\"engtext\"])\n",
    "evaluation_dataset['h_adj_engtext'], evaluation_dataset['h_nat_engtext'] = heuristic_from_text(eval_engtext)\n",
    "evaluation_dataset.dropna(subset=[\"h_adj_engtext\", \"h_nat_engtext\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a0628d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now informations on engtext are divided by quantili\n",
    "# n_quantili_engtext = 4\n",
    "# adj_engtext_bins_edges = pd.qcut(dataset['h_adj_engtext'], q=n_quantili_engtext, retbins=True, duplicates='drop')[1]\n",
    "# nat_engtext_bins_edges = pd.qcut(dataset['h_nat_engtext'], q=n_quantili_engtext, retbins=True, duplicates='drop')[1]\n",
    "\n",
    "# dataset['adj_engtext_bins'] = pd.cut(dataset['h_adj_engtext'], bins=adj_engtext_bins_edges, include_lowest=True)\n",
    "# dataset['nat_engtext_bins'] = pd.cut(dataset['h_nat_engtext'], bins=nat_engtext_bins_edges, include_lowest=True)\n",
    "\n",
    "# evaluation_dataset['adj_engtext_bins'] = pd.cut(evaluation_dataset['h_adj_engtext'], bins=adj_engtext_bins_edges, include_lowest=True)\n",
    "# evaluation_dataset['nat_engtext_bins'] = pd.cut(evaluation_dataset['h_nat_engtext'], bins=nat_engtext_bins_edges, include_lowest=True)\n",
    "\n",
    "# #Now drop the original columns\n",
    "# dataset = dataset.drop(columns=[\"h_adj_engtext\", \"h_nat_engtext\"])\n",
    "# evaluation_dataset = evaluation_dataset.drop(columns=[\"h_adj_engtext\", \"h_nat_engtext\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2e9bfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename(columns={\"std\": \"std_text\", \"avg\": \"avg_text\", \"distribution\": \"text_distribution\"}, inplace=True)\n",
    "evaluation_dataset.rename(columns={\"std\": \"std_text\", \"avg\": \"avg_text\", \"distribution\": \"text_distribution\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bd4f5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6193\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(evaluation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1145f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"datasets/silver_unicum.csv\", mode='w', index=False)\n",
    "evaluation_dataset.to_csv(\"datasets/gold_unicum.csv\", mode='w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
