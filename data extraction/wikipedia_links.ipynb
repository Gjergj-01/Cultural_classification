{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from wikidata.client import Client\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ecb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the two dataset to csv\n",
    "silverdataset = pd.read_csv('silver.csv')\n",
    "golddataset = pd.read_csv('gold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1070101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "def extract_wikipedia_links(entity_id):\n",
    "    client = Client()\n",
    "    print(extract_entity_id(entity_id))\n",
    "    item = client.get(extract_entity_id(entity_id), load=True)\n",
    "    sitelinks = item.data.get(\"sitelinks\", {})\n",
    "\n",
    "    # Filter only Wikipedia sitelinks and print them\n",
    "    #print(\"Wikipedia pages available for this entity:\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for site_key, site_data in sitelinks.items():\n",
    "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
    "            lang = site_key.replace(\"wiki\", \"\")\n",
    "            title = site_data[\"title\"]\n",
    "            url = f\"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "            #print(f\"{lang}: {url}\")\n",
    "            results.append([entity_id, lang, url])\n",
    "    return pd.DataFrame(results, columns=['entity', 'language', 'url'])\n",
    "\n",
    "# Funzione per salvare progressivamente i risultati in un file CSV\n",
    "def save_wikipedia_links(file_name):\n",
    "    output_file_name = \"wikipedia_links_\"+file_name\n",
    "    # Controlla se il file di output esiste già\n",
    "    if os.path.exists(output_file_name):\n",
    "        # Carica il file esistente\n",
    "        existing_df = pd.read_csv(output_file_name)\n",
    "        # Raccoglie le keyword già trattate\n",
    "        processed_keywords = existing_df['entity'].unique().tolist()\n",
    "        print(len(processed_keywords), \"keywords già trattate.\")\n",
    "    else:\n",
    "        #Creo un nuovo dataframe\n",
    "        existing_df = pd.DataFrame(columns=['entity', 'language', 'url'])\n",
    "        processed_keywords = []\n",
    "    \n",
    "    # Carica il dataset di input\n",
    "    data = pd.read_csv(file_name)\n",
    "    df = pd.DataFrame(data, columns=[\"item\"]) #item = wikidata link\n",
    "\n",
    "    # Lista per raccogliere i risultati\n",
    "    results = []\n",
    "\n",
    "    # Cicla su ogni entità\n",
    "    for idx, row in df.iterrows():\n",
    "        keyword = row[\"item\"]\n",
    "        \n",
    "        # Se la keyword è già stata trattata, salta\n",
    "        if keyword in processed_keywords:\n",
    "            print(keyword,\" già trattata, salto.\")\n",
    "            continue\n",
    "        \n",
    "        #time.sleep(random.randint(2, 6))\n",
    "        print(\"Elaboro: \", keyword)\n",
    "        #Extract data\n",
    "        trend_data = extract_wikipedia_links(keyword)\n",
    "        # Aggiungi i nuovi risultati se non sono vuoti\n",
    "        if not trend_data.empty:\n",
    "            results.append(trend_data)\n",
    "        \n",
    "        # Scrivi progressivamente i risultati\n",
    "        if results:\n",
    "            new_results = pd.concat(results, ignore_index=True)\n",
    "            new_results.to_csv(output_file_name, mode='a', index=False)\n",
    "            results = []  # Resetta la lista dei risultati per non aggiungere più volte\n",
    "        \n",
    "        # Aggiungi la keyword alla lista delle trattate\n",
    "        processed_keywords.append(keyword)\n",
    "\n",
    "    print(\"Processing completato\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_wikipedia_links('silver.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
